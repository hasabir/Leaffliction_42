{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc20ec83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "from img2vec_pytorch import Img2Vec\n",
    "from PIL import Image\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import  models, layers\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2543c54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "# BATCH_SIZE = 32\n",
    "# IMAGE_SIZE = 128\n",
    "IMAGE_SIZE = 256\n",
    "CHANNELS=3\n",
    "\n",
    "EPOCHS=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfc830a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_partitions_tf(ds, train_split=0.8, val_split=0.1, test_split=0.1, shuffle=True, shuffle_size=10000):\n",
    "    assert (train_split + test_split + val_split) == 1\n",
    "    \n",
    "    ds_size = len(ds)\n",
    "    \n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(shuffle_size, seed=12)\n",
    "    \n",
    "    train_size = int(train_split * ds_size)\n",
    "    val_size = int(val_split * ds_size)\n",
    "    \n",
    "    train_ds = ds.take(train_size)    \n",
    "    val_ds = ds.skip(train_size).take(val_size)\n",
    "    test_ds = ds.skip(train_size).skip(val_size)\n",
    "    \n",
    "    return train_ds, val_ds, test_ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fd45a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data_dir = \"augmented_directory/Appleles\"\n",
    "# def train(data_dir):\n",
    "dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    seed=123,\n",
    "    shuffle=True,\n",
    "    image_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    batch_size=BATCH_SIZE)\n",
    "\n",
    "class_names = dataset.class_names\n",
    "train_dataset, val_dataset, test_dataset = get_dataset_partitions_tf(dataset)\n",
    "\n",
    "# FIX: Remove BATCH_SIZE from input_shape - it should only have (height, width, channels)\n",
    "input_shape = (IMAGE_SIZE, IMAGE_SIZE, CHANNELS)\n",
    "\n",
    "resize_and_rescale = tf.keras.Sequential([\n",
    "    layers.Rescaling(1.0 / 255),  # Resizing is already done by image_dataset_from_directory\n",
    "])\n",
    "\n",
    "n_classes = len(class_names)\n",
    "\n",
    "model = models.Sequential([\n",
    "    resize_and_rescale,\n",
    "    layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(n_classes, activation='softmax'),\n",
    "])\n",
    "\n",
    "# FIX: Use None for batch dimension when building\n",
    "model.build(input_shape=(None, IMAGE_SIZE, IMAGE_SIZE, CHANNELS))\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    # Remove batch_size here - it's already batched in the dataset\n",
    "    validation_data=val_dataset,\n",
    "    verbose=1,\n",
    "    epochs=EPOCHS,  # Use the EPOCHS constant you defined\n",
    ")\n",
    "    # return history\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
